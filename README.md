# COMP7607 ÔΩú Natural Language Processing ÔΩú Fall 2024
https://nlp.cs.hku.hk/comp7607-fall2024/

This is the course material for HKU COMP7607

## Course description
Natural language processing (NLP) is the study of human language from a computational perspective. The course will be focusing on machine learning and corpus-based methods and algorithms. We will cover syntactic, semantic and discourse processing models. We will describe the use of these methods and models in applications including syntactic parsing, information extraction, statistical machine translation, dialogue systems, and summarization. This course starts with language models (LMs), which are both front and center in natural language processing (NLP), and then introduces key machine learning (ML) ideas that students should grasp (e.g. feature-based models, log-linear models and then the neural models). We will land on modern generic meaning representation methods (e.g. BERT/GPT-3) and the idea of pretraining / finetuning.

## About
A research project exploring effective prompt engineering techniques for Large Language Models (LLMs) to enhance performance and accuracy.

### üîç Key Findings
Performance & Methods
- Self-Refine method demonstrates superior standalone performance
- Combined optimization methods improve baseline accuracy but don't exceed Self-Refine's individual results
- Iterative approaches increase accuracy with tradeoff in inference time
- External guidance signals outperform self-referential outputs
Optimal Prompt Design
- Solution structure impacts accuracy more than final answers
- Sweet spot for demonstrations: 10-15 examples
- Quality over quantity in prompt construction
- Moderate complexity yields better results than overly simple or complex prompts
Best Practices
- Maintain consistent prompt styling
- Use rigorous formatting for mathematical problems
- Include necessary context without redundancy
- Structure prompts with clear logical flow

### üí° Practical Applications
This research provides actionable insights for:
- Mathematical problem-solving
- LLM response optimization
- Prompt design methodology
- Performance tuning

### ‚öñÔ∏è Implementation Considerations
When implementing these findings, consider:
- Balancing accuracy vs. generation speed
- Resource constraints
- Specific use case requirements
- Computational costs

### üöÄ Future Directions
Areas for potential exploration:
- Automated prompt optimization
- Domain-specific prompt patterns
- Efficiency improvements
- Cross-model compatibility
